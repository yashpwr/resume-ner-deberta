model:
  name: "bert-base-uncased"  # Changed from deberta-v3-small to avoid tokenizer issues in Kaggle
  max_length: 192 # was 256, shorter for Kaggle wall time
  stride: 32

training:
  learning_rate: 3e-4
  batch_size: 4
  grad_accum_steps: 4
  num_epochs: 5
  early_stopping_patience: 3
  warmup_steps: 100
  weight_decay: 0.01
  fp16: true
  label_smoothing: 0.05
  # NEW: Kaggle-resume friendly
  save_steps: 500
  save_total_limit: 3
  eval_steps: 500
  evaluation_strategy: "steps"
  logging_steps: 50
  gradient_checkpointing: true

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["query", "value"]

data:
  train_file: "data/train.jsonl"
  val_file: "data/val.jsonl"
  test_file: "data/test.jsonl"

output:
  model_dir: "artifacts/model"
  logs_dir: "artifacts/logs"

seed: 42
